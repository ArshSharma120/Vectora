// background.js - Standalone service worker for Vectora AI Check Extension
// ULTRA USELESS CONCEPT OF USING SERVER TO USE A EXTENSION.
// No backend server required - makes direct API calls to AI providers

// Settings
let settings = {
  provider: 'gemini',
  cerebras_api_key: '',
  cerebras_model: 'llama-3.3-70b',
  gemini_api_key: '',
  gemini_model: 'gemini-2.0-flash-exp',
  groq_api_key: '',
  groq_model: 'llama-3.3-70b-versatile'
};

// Load settings on startup
loadSettings();

function loadSettings() {
  chrome.storage.sync.get([
    'provider',
    'cerebras_api_key',
    'cerebras_model',
    'gemini_api_key',
    'gemini_model',
    'groq_api_key',
    'groq_model'
  ], (result) => {
    settings = { ...settings, ...result };
    console.log('Settings loaded:', { ...settings, cerebras_api_key: '***', gemini_api_key: '***', groq_api_key: '***' });
  });
}

// Listen for settings updates
chrome.runtime.onMessage.addListener((message, sender, sendResponse) => {
  if (message.action === 'settingsUpdated') {
    loadSettings();
  }
});

// Initialize context menus
chrome.runtime.onInstalled.addListener(() => {
  chrome.contextMenus.removeAll(() => {
    // Text selection menu
    chrome.contextMenus.create({
      id: 'vectora-check-text',
      title: 'Check AI Authenticity',
      contexts: ['selection']
    });

    // Image menu
    chrome.contextMenus.create({
      id: 'vectora-check-image',
      title: 'Check AI Authenticity',
      contexts: ['image']
    });
  });
});

// Handle context menu clicks
chrome.contextMenus.onClicked.addListener(async (info, tab) => {
  try {
    if (info.menuItemId === 'vectora-check-text') {
      const selectedText = info.selectionText || '';

      if (!selectedText.trim()) {
        showNotification(tab.id, 0, 'Please select text to analyze');
        return;
      }

      // Check if API key is configured
      const apiKey = settings[`${settings.provider}_api_key`];
      if (!apiKey) {
        showNotification(tab.id, 0, 'Please configure API key in extension settings');
        return;
      }

      showNotification(tab.id, 0, 'Analyzing text...');

      const result = await analyzeText(selectedText);

      showNotification(tab.id, result.ai_percent, result.message);
      chrome.storage.local.set({ lastCheck: result });
    }

    else if (info.menuItemId === 'vectora-check-image') {
      const srcUrl = info.srcUrl || '';

      if (!srcUrl) {
        showNotification(tab.id, 0, 'Could not extract image URL');
        return;
      }

      const apiKey = settings[`${settings.provider}_api_key`];
      if (!apiKey) {
        showNotification(tab.id, 0, 'Please configure API key in extension settings');
        return;
      }

      showNotification(tab.id, 0, 'Analyzing image...');

      const result = await analyzeImage(srcUrl);

      showNotification(tab.id, result.ai_percent, result.message);
      chrome.storage.local.set({ lastCheck: result });
    }
  } catch (error) {
    console.error('Context menu error:', error);
    showNotification(tab.id, 0, `Error: ${error.message}`);
  }
});

// Analyze text using selected AI provider
async function analyzeText(text) {
  const provider = settings.provider;
  const apiKey = settings[`${provider}_api_key`];
  const model = settings[`${provider}_model`];

  const prompt = `Analyze this text and determine the likelihood it was generated by AI.

Text to analyze:
${text}

Respond with ONLY a JSON object (no markdown, no extra text):
{"ai_percent": <0-100>, "reason": "<brief explanation>"}

Consider: writing style, unusual patterns, perfect grammar, repetitive phrases, lack of human emotion/opinions, generic content.
Return a percentage 0-100 where:
- 0-20%: Clearly human written
- 21-40%: Likely human with possible AI assistance
- 41-60%: Mixed or unclear
- 61-80%: Likely AI-generated
- 81-100%: Almost certainly AI-generated`;

  try {
    let response;

    if (provider === 'cerebras') {
      response = await callCerebrasAPI(apiKey, model, prompt);
    } else if (provider === 'gemini') {
      response = await callGeminiAPI(apiKey, model, prompt);
    } else if (provider === 'groq') {
      response = await callGroqAPI(apiKey, model, prompt);
    }

    return parseAIResponse(response);
  } catch (error) {
    console.error('Analysis error:', error);
    return {
      ai_percent: 50,
      message: `Error: ${error.message}`
    };
  }
}

// Analyze image using selected AI provider
async function analyzeImage(imageUrl) {
  const provider = settings.provider;
  const apiKey = settings[`${provider}_api_key`];
  const model = settings[`${provider}_model`];

  const prompt = `Analyze this image and determine the likelihood it was AI-generated.

Respond with ONLY a JSON object (no markdown, no extra text):
{"ai_percent": <0-100>, "reason": "<brief explanation>"}

Consider: artifacts, unnatural patterns, weird textures, impossible physics, watermarks, AI tool signatures.
Return 0-100 where 0=clearly real, 100=certainly AI-generated.`;

  try {
    // Download image and convert to base64
    const imageData = await downloadImage(imageUrl);

    let response;

    if (provider === 'gemini') {
      response = await callGeminiVisionAPI(apiKey, model, prompt, imageData);
    } else if (provider === 'groq') {
      // Check if model supports vision
      if (!model.includes('vision')) {
        return {
          ai_percent: 50,
          message: 'Selected Groq model does not support image analysis. Please use a vision model.'
        };
      }
      response = await callGroqVisionAPI(apiKey, model, prompt, imageData);
    } else {
      return {
        ai_percent: 50,
        message: 'Cerebras does not support image analysis. Please select Gemini or Groq with a vision model.'
      };
    }

    return parseAIResponse(response);
  } catch (error) {
    console.error('Image analysis error:', error);
    return {
      ai_percent: 50,
      message: `Error: ${error.message}`
    };
  }
}

// Download image and convert to base64
async function downloadImage(url) {
  try {
    const response = await fetch(url);
    const blob = await response.blob();
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        const base64 = reader.result.split(',')[1];
        resolve({ data: base64, mimeType: blob.type });
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  } catch (error) {
    throw new Error(`Failed to download image: ${error.message}`);
  }
}

// API Calls

async function callCerebrasAPI(apiKey, model, prompt) {
  const response = await fetch('https://api.cerebras.ai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.3,
      max_tokens: 256
    })
  });

  if (!response.ok) {
    throw new Error(`Cerebras API error: ${response.status}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

async function callGeminiAPI(apiKey, model, prompt, imageData = null) {
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;

  const parts = [{ text: prompt }];

  const payload = {
    contents: [{ parts }],
    generationConfig: { temperature: 0.3 }
  };

  const response = await fetch(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(payload)
  });

  if (!response.ok) {
    throw new Error(`Gemini API error: ${response.status}`);
  }

  const data = await response.json();
  return data.candidates[0].content.parts[0].text;
}

async function callGeminiVisionAPI(apiKey, model, prompt, imageData) {
  const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;

  const parts = [
    { text: prompt },
    {
      inline_data: {
        mime_type: imageData.mimeType,
        data: imageData.data
      }
    }
  ];

  const payload = {
    contents: [{ parts }],
    generationConfig: { temperature: 0.3 }
  };

  const response = await fetch(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(payload)
  });

  if (!response.ok) {
    throw new Error(`Gemini API error: ${response.status}`);
  }

  const data = await response.json();
  return data.candidates[0].content.parts[0].text;
}

async function callGroqAPI(apiKey, model, prompt) {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: 'user', content: prompt }],
      temperature: 0.3,
      max_tokens: 256
    })
  });

  if (!response.ok) {
    throw new Error(`Groq API error: ${response.status}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

async function callGroqVisionAPI(apiKey, model, prompt, imageData) {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{
        role: 'user',
        content: [
          { type: 'text', text: prompt },
          {
            type: 'image_url',
            image_url: {
              url: `data:${imageData.mimeType};base64,${imageData.data}`
            }
          }
        ]
      }],
      temperature: 0.3,
      max_tokens: 256
    })
  });

  if (!response.ok) {
    throw new Error(`Groq API error: ${response.status}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

// Parse AI response to extract percentage and reason
function parseAIResponse(text) {
  try {
    // Clean markdown code blocks
    text = text.replace(/```json/g, '').replace(/```/g, '').trim();

    // Try to find JSON in response
    const jsonMatch = text.match(/\{.*?\}/s);
    if (jsonMatch) {
      const json = JSON.parse(jsonMatch[0]);
      return {
        ai_percent: Math.min(100, Math.max(0, parseInt(json.ai_percent) || 50)),
        message: json.reason || 'Analysis complete'
      };
    }

    // Fallback: extract from text
    const percentMatch = text.match(/(\d+)%/);
    const ai_percent = percentMatch ? parseInt(percentMatch[1]) : 50;

    return {
      ai_percent: Math.min(100, Math.max(0, ai_percent)),
      message: text.substring(0, 100)
    };
  } catch (error) {
    return {
      ai_percent: 50,
      message: 'Unable to parse response'
    };
  }
}

// Show notification overlay
function showNotification(tabId, aiPercent, message) {
  chrome.scripting.executeScript({
    target: { tabId },
    func: injectNotification,
    args: [aiPercent, message]
  }).catch(err => console.error('Notification error:', err));
}

// Injected notification function
function injectNotification(percent, msg) {
  const existing = document.getElementById('vectora-notification');
  if (existing) existing.remove();

  if (!document.getElementById('vectora-styles')) {
    const style = document.createElement('style');
    style.id = 'vectora-styles';
    style.textContent = `
      #vectora-notification {
        position: fixed;
        top: 24px;
        right: 24px;
        z-index: 999999;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
        border: 1px solid rgba(0, 243, 255, 0.3);
        border-radius: 12px;
        padding: 16px 20px;
        min-width: 280px;
        max-width: 320px;
        box-shadow: 0 8px 32px rgba(0, 243, 255, 0.15), 0 0 20px rgba(0, 0, 0, 0.5);
        font-family: 'Segoe UI', Tahoma, Geneva, sans-serif;
        color: #e0e0e0;
        backdrop-filter: blur(8px);
        animation: vectoraSlideIn 0.4s cubic-bezier(0.34, 1.56, 0.64, 1);
      }
      @keyframes vectoraSlideIn {
        from {
          opacity: 0;
          transform: translateX(360px) translateY(-20px);
        }
        to {
          opacity: 1;
          transform: translateX(0) translateY(0);
        }
      }
      .vectora-ai-percent {
        font-size: 32px;
        font-weight: 900;
        color: #00f3ff;
        text-shadow: 0 0 10px rgba(0, 243, 255, 0.5);
        margin-bottom: 6px;
        letter-spacing: -1px;
      }
      .vectora-label {
        font-size: 11px;
        color: #888;
        text-transform: uppercase;
        letter-spacing: 1px;
        margin-bottom: 10px;
        font-weight: 600;
      }
      .vectora-message {
        font-size: 13px;
        color: #b0b0b0;
        margin-bottom: 12px;
        line-height: 1.4;
        word-break: break-word;
      }
      .vectora-powered {
        font-size: 10px;
        color: #666;
        text-align: right;
        font-weight: 500;
        letter-spacing: 0.5px;
      }
    `;
    document.head.appendChild(style);
  }

  const notif = document.createElement('div');
  notif.id = 'vectora-notification';
  notif.innerHTML = `
    <div class="vectora-label">AI Involvement</div>
    <div class="vectora-ai-percent">${Math.round(percent)}%</div>
    <div class="vectora-message">${String(msg).substring(0, 100)}</div>
    <div class="vectora-powered">Powered by Vectora</div>
  `;
  document.body.appendChild(notif);

  setTimeout(() => {
    notif.style.opacity = '0';
    notif.style.transition = 'opacity 0.3s ease-out';
    setTimeout(() => {
      if (notif.parentNode) notif.remove();
    }, 300);
  }, 5000);
}
